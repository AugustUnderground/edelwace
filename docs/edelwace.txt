-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | HaskTorch Reinforcement Learning Agents for GACE
--   
--   Please see the README on GitHub at
--   <a>https://github.com/augustunderground/edelwace#readme</a>
@package edelwace
@version 0.1.0.0


-- | Utility and Helper functions for EDELWACE
module Lib

-- | Swaps the arguments of HaskTorch's foldLoop around
foldLoop' :: Int -> (a -> Int -> IO a) -> a -> IO a

-- | Because snake_case sucks
nanToNum :: Float -> Float -> Float -> Tensor -> Tensor

-- | Default limits for <a>nanToNum</a>
nanToNum' :: Tensor -> Tensor

-- | Default limits for <a>nanToNum</a> (0.0)
nanToNum'' :: Tensor -> Tensor

-- | GPU Tensor filled with Float value
fullLike' :: Tensor -> Float -> Tensor

-- | Select index with [Int] from GPU tensor
indexSelect'' :: Int -> [Int] -> Tensor -> Tensor

-- | Calculate weight Limits based on Layer Dimensions
weightLimit :: Linear -> Float

-- | Initialize Weights of Linear Layer
weightInit :: Float -> Linear -> IO Linear

-- | Initialize weights based on Fan In
weightInit' :: Linear -> IO Linear

-- | Softly update parameters from Online Net to Target Net
softUpdate :: Tensor -> Tensor -> Tensor -> Tensor

-- | Softly copy parameters from Online Net to Target Net
softSync :: Parameterized f => Tensor -> f -> f -> IO f

-- | Hard Copy of Parameter from one net to the other
copySync :: Parameterized f => f -> f -> f

-- | GPU 1
gpu :: Device

-- | CPU 0
cpu :: Device

-- | Default Tensor Data Type
dataType :: DType

-- | Convert an Array to a Tensor
toTensor :: TensorLike a => a -> Tensor

-- | Convert an Array to a Tensor
toIntTensor :: TensorLike a => a -> Tensor

-- | Create an empty Float Tensor on GPU
emptyTensor :: Tensor

-- | Convert a Scalar to a Tensor
toScalar :: Float -> Tensor

-- | Convert model to Double on GPU
toDoubleGPU :: forall a. HasTypes a Tensor => a -> a

-- | Convert model to Float on CPU
toFloatGPU :: forall a. HasTypes a Tensor => a -> a

-- | Convert model to Float on CPU
toFloatCPU :: forall a. HasTypes a Tensor => a -> a

-- | Generate a Tensor of random Integers
randomInts :: Int -> Int -> Int -> IO Tensor

-- | Generate Normally Distributed Random values given dimensions
normal' :: [Int] -> IO Tensor

-- | Generate Normally Distributed Random values given μs and σs
normal :: Tensor -> Tensor -> IO Tensor

-- | Info object gotten form stepping
data Info
Info :: ![String] -> ![String] -> Info

-- | Observation Keys
[observations] :: Info -> ![String]

-- | Action Keys
[actions] :: Info -> ![String]

-- | Single Environment Step
data Step
Step :: ![Float] -> !Float -> !Bool -> !Info -> Step

-- | Observation Vector
[observation] :: Step -> ![Float]

-- | Reward Scalar
[reward] :: Step -> !Float

-- | Terminal Indicator
[done] :: Step -> !Bool

-- | Info
[info] :: Step -> !Info

-- | Base Route to Hym Server
type HymURL = String

-- | Possible Action Spaces
data ActionSpace

-- | Continuous Action Space
Continuous :: ActionSpace

-- | Discrete Action Space
Discrete :: ActionSpace

-- | Convert a Map to a Tensor where Pool index is a dimension
mapToTensor :: Map Int [Float] -> Tensor

-- | Convert Tensor to Map (Continuous action spaces)
tensorToMap :: Tensor -> Map Int [Float]

-- | Convert Tensor to Map (Discrete action spaces)
tensorToMap' :: Tensor -> Map Int Int

-- | Convert the Pooled Step Map to a Tuple
stepsToTuple :: Map Int Step -> (Tensor, Tensor, Tensor, [Info])

-- | Generic HTTP GET Request to Hym Server
hymGet :: HymURL -> String -> IO ByteString

-- | Send a POST Request to a Hym Server
hymPost :: HymURL -> String -> Value -> IO ByteString

-- | Convert a JSON Response from an ACE Server to a Map
hymPoolMap :: HymURL -> String -> IO (Map Int (Map String Float))

-- | Convert a JSON Response from an ACE Server to a Float-List
hymPoolList :: HymURL -> String -> IO (Map Int [Float])

-- | Convert a JSON Response from an ACE Server to a String-List
hymPoolList' :: HymURL -> String -> IO (Map Int [String])

-- | Reset Pooled Environments on a Hym server
hymPoolReset :: HymURL -> IO (Map Int [Float])

-- | Get Random Actions from all Pooled Environments
hymPoolRandomAction :: HymURL -> IO (Map Int [Float])

-- | Perform Random Actions in all Pooled Environments
hymPoolRandomStep :: HymURL -> IO (Map Int Step)

-- | Take Steps in All Environments (Continuous)
hymPoolStep :: HymURL -> Map Int [Float] -> IO (Map Int Step)

-- | Take Steps in All Environments (Discrete)
hymPoolStep' :: HymURL -> Map Int Int -> IO (Map Int Step)

-- | Generate URL to a Hym-GACE server from meta information
aceURL :: String -> String -> String -> String -> String -> HymURL

-- | Generate URL to a Hym-Gym server from meta information
gymURL :: String -> String -> String -> String -> HymURL

-- | Send a GET Request to a GACE Server Obtain the Target of Pooled GACE
--   Environments
acePoolTarget :: HymURL -> IO (Map Int (Map String Float))

-- | Action Keys from GACE Server
acePoolActKeys :: HymURL -> IO (Map Int [String])

-- | Observation Keys from GACE Server
acePoolObsKeys :: HymURL -> IO (Map Int [String])

-- | Get the SHACE logging path as a dict
shaceLogPath' :: HymURL -> IO (Map String String)

-- | Get the SHACE logging path
shaceLogPath :: HymURL -> IO String

-- | Reset a Vectorized Environment Pool
resetPool :: HymURL -> IO Tensor

-- | Reset selected Environments from Pool
resetPool' :: HymURL -> Tensor -> IO Tensor

-- | Shorthand for getting keys of pooled same envs
actKeysPool :: HymURL -> IO [String]

-- | Shorthand for getting keys of pooled same envs
obsKeysPool :: HymURL -> IO [String]

-- | Number of Environments in Pool
numEnvsPool :: HymURL -> IO Int

-- | Get Info without stepping
infoPool :: HymURL -> IO Info

-- | Step in a Control Environment
stepPool :: HymURL -> Tensor -> IO (Tensor, Tensor, Tensor, [Info])

-- | Step in a Discrete Environment
stepPool' :: HymURL -> Tensor -> IO (Tensor, Tensor, Tensor, [Info])

-- | Take a random Step an Environment
randomStepPool :: HymURL -> IO (Tensor, Tensor, Tensor, [Info])

-- | Get a set of random actions from the current environment
randomActionPool :: HymURL -> IO Tensor

-- | Optimizer moments at given prefix
saveOptim :: Adam -> FilePath -> IO ()

-- | Load Optimizer State
loadOptim :: Int -> Float -> Float -> FilePath -> IO Adam

-- | Create Boolean Mask Tensor from list of indices.
boolMask :: Int -> [Int] -> Tensor

-- | Process / Sanitize the Observations from GACE
processGace :: Tensor -> Info -> Tensor

-- | Scale reward to center
scaleRewards :: Tensor -> Float -> Tensor

-- | Sanatize JSON for MLFlow: Names may only contain alphanumerics,
--   underscores (_), dashes (-), periods (.), spaces ( ), and slashes (/).
sanatizeJSON :: Char -> Char

-- | Data Logging to MLFlow Trackign Server
data Tracker
Tracker :: TrackingURI -> ExperimentID -> String -> Map String RunID -> Tracker

-- | Tracking Server URI
[uri] :: Tracker -> TrackingURI

-- | Experiment ID
[experimentId] :: Tracker -> ExperimentID

-- | Experiment Name
[experimentName] :: Tracker -> String

-- | Run IDs
[runIds] :: Tracker -> Map String RunID

-- | Retrieve a run ID
runId :: Tracker -> String -> RunID

-- | Make new Tracker given a Tracking Server URI
mkTracker :: TrackingURI -> String -> IO Tracker

-- | Make new Tracker given a Hostname and Port
mkTracker' :: String -> Int -> String -> IO Tracker

-- | Create a new Experiment with rng suffix
newExperiment :: Tracker -> String -> IO Tracker

-- | Create a new Experiment
newExperiment' :: Tracker -> String -> IO Tracker

-- | Create a new run with a set of given paramters
newRuns :: Tracker -> [String] -> [Param] -> IO Tracker

-- | New run with algorithm id and #envs as log params
newRuns' :: Int -> Tracker -> IO Tracker

-- | End a run
endRun :: String -> Tracker -> IO Tracker

-- | End all runs of a Tracker
endRuns :: Tracker -> IO Tracker

-- | End all runs and discard tracker
endRuns' :: Tracker -> IO ()

-- | Write Loss to Tracking Server
trackLoss :: Tracker -> Int -> String -> Float -> IO (Response ByteString)

-- | Write Reward to Tracking Server
trackReward :: Tracker -> Int -> Tensor -> IO ()

-- | Filter Performance of all envs
filterPerformance :: Map Int (Map String Float) -> [String] -> Map Int (Map String Float)

-- | Write Current state of the Environment to Trackign Server
trackEnvState :: Tracker -> HymURL -> Int -> IO ()
instance GHC.Show.Show Lib.Info
instance GHC.Generics.Generic Lib.Info
instance GHC.Show.Show Lib.Step
instance GHC.Generics.Generic Lib.Step
instance GHC.Classes.Eq Lib.ActionSpace
instance GHC.Show.Show Lib.ActionSpace
instance GHC.Show.Show Lib.Tracker
instance Data.Aeson.Types.FromJSON.FromJSON Lib.Step
instance Data.Aeson.Types.ToJSON.ToJSON Lib.Step
instance Data.Aeson.Types.FromJSON.FromJSON Lib.Info
instance Data.Aeson.Types.ToJSON.ToJSON Lib.Info

module Normal

-- | Creates a normal (also called Gaussian) distribution parameterized by
--   <a>loc</a> and <a>scale</a>. Example:
--   
--   <pre>
--   &gt;&gt;&gt; m = Normal (asTensor ([0.0] :: [Float])) (asTensor ([1.0] :: [Float]))
--   
--   &gt;&gt;&gt; sample m []
--   Tensor Float [1] [-0.1205   ]
--   </pre>
data Normal
Normal :: Tensor -> Tensor -> Normal

-- | mean of the distribution (often referred to as mu)
[loc] :: Normal -> Tensor

-- | standard deviation of the distribution (often referred to as sigma)
[scale] :: Normal -> Tensor

-- | PDF for given Normal Distribution
prob :: Normal -> Tensor -> Tensor
instance GHC.Show.Show Normal.Normal
instance Torch.Distributions.Distribution.Distribution Normal.Normal


-- | Proximal Policy Optimization Algorithm Defaults
module PPO.Defaults

-- | Algorithm ID
algorithm :: String

-- | Print verbose debug output
verbose :: Bool

-- | Number of episodes to play
numEpisodes :: Int

-- | How many steps to take in env
numSteps :: Int

-- | How many gradient update steps
numEpochs :: Int

-- | Number of iterations
numIterations :: Int

-- | Early stop criterion
earlyStop :: Tensor

-- | Size of the batches during epoch
batchSize :: Int

-- | Random seed for reproducability
rngSeed :: Int

-- | Maximum time to cut off
maxTime :: Float

-- | ACE Identifier of the Environment
aceId :: String

-- | PDK/Technology backend of the ACE Environment
aceBackend :: String

-- | ACE Environment variant
aceVariant :: Int

-- | Discrete or Continuous action space
actionSpace :: ActionSpace

-- | Scale for reward centering
rewardScale :: Float

-- | Factor for clipping
ε :: Float

-- | Factor in loss function
δ :: Tensor

-- | Discount Factor
γ :: Tensor

-- | Avantage Factor
τ :: Tensor

-- | Initial weights
wInit :: Float

-- | Learning Rate
η :: Tensor

-- | Betas
β1 :: Float

-- | Betas
β2 :: Float


-- | Replay Buffers and Memory Loaders
module RPB

-- | Indicate Buffer Type
data Buffer

-- | Normal Replay Buffer
RPB :: Buffer

-- | Prioritized Experience Replay
PER :: Buffer

-- | PPO Style replay Memory
MEM :: Buffer

-- | Strict Simple/Naive Replay Buffer
data ReplayBuffer a
ReplayBuffer :: !a -> !a -> !a -> !a -> !a -> ReplayBuffer a

-- | States
[rpbStates] :: ReplayBuffer a -> !a

-- | Actions
[rpbActions] :: ReplayBuffer a -> !a

-- | Rewards
[rpbRewards] :: ReplayBuffer a -> !a

-- | Next States
[rpbStates'] :: ReplayBuffer a -> !a

-- | Terminal Mask
[rpbDones] :: ReplayBuffer a -> !a

-- | Create a new, empty Buffer on the CPU
mkBuffer :: ReplayBuffer Tensor

-- | How many Trajectories are currently stored in memory
bufferLength :: ReplayBuffer Tensor -> Int

-- | Push new memories into Buffer
bufferPush :: Int -> ReplayBuffer Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> ReplayBuffer Tensor

-- | Pushing one buffer into another one
bufferPush' :: Int -> ReplayBuffer Tensor -> ReplayBuffer Tensor -> ReplayBuffer Tensor

-- | Get the given indices from Buffer
bufferSample :: Tensor -> ReplayBuffer Tensor -> ReplayBuffer Tensor

-- | Uniform random sample from Replay Buffer
bufferRandomSample :: Int -> ReplayBuffer Tensor -> IO (ReplayBuffer Tensor)

-- | Strict Prioritized Experience Replay Buffer
data PERBuffer a
PERBuffer :: ReplayBuffer a -> !Tensor -> !Int -> !Float -> !Float -> !Int -> PERBuffer a

-- | Actual Buffer
[perMemories] :: PERBuffer a -> ReplayBuffer a

-- | Sample Weights
[perPriorities] :: PERBuffer a -> !Tensor

-- | Buffer Capacity
[perCapacity] :: PERBuffer a -> !Int

-- | Exponent Alpha
[perAlpha] :: PERBuffer a -> !Float

-- | Initial Exponent Beta
[perBetaStart] :: PERBuffer a -> !Float

-- | Beta Decay
[perBetaFrames] :: PERBuffer a -> !Int

-- | Create an empty PER Buffer
mkPERBuffer :: Int -> Float -> Float -> Int -> PERBuffer Tensor

-- | Push new memories in a Buffer
perPush :: PERBuffer Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> PERBuffer Tensor

-- | Syntactic Sugar for adding one buffer to another
perPush' :: PERBuffer Tensor -> PERBuffer Tensor -> PERBuffer Tensor

-- | Take a prioritized sample from the Buffer
perSample :: PERBuffer Tensor -> Int -> Int -> IO (ReplayBuffer Tensor, Tensor, Tensor)

-- | Update the Priorities of a Buffer
perUpdate :: PERBuffer Tensor -> Tensor -> Tensor -> PERBuffer Tensor

-- | Calculate the β exponent at a given frame
betaByFrame :: Float -> Int -> Int -> Float

-- | Replay Memory
data ReplayMemory a
ReplayMemory :: !a -> !a -> !a -> !a -> !a -> !a -> ReplayMemory a

-- | States
[memStates] :: ReplayMemory a -> !a

-- | Action
[memActions] :: ReplayMemory a -> !a

-- | Logarithmic Probability
[memLogPorbs] :: ReplayMemory a -> !a

-- | Rewards
[memRewards] :: ReplayMemory a -> !a

-- | Values
[memValues] :: ReplayMemory a -> !a

-- | Terminal Mask
[memMasks] :: ReplayMemory a -> !a

-- | Create a new, empty Buffer on the CPU
mkMemory :: ReplayMemory Tensor

-- | How many Trajectories are currently stored in memory
memoryLength :: ReplayMemory Tensor -> Int

-- | Push new memories into Buffer
memoryPush :: ReplayMemory Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> ReplayMemory Tensor

-- | Pushing one buffer into another one
memoryPush' :: ReplayMemory Tensor -> ReplayMemory Tensor -> ReplayMemory Tensor

-- | Generalized Advantage Estimate
gae :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor

-- | Memory Data Loader
data MemoryLoader a
MemoryLoader :: !a -> !a -> !a -> !a -> !a -> MemoryLoader a

-- | States
[loaderStates] :: MemoryLoader a -> !a

-- | Actions
[loaderActions] :: MemoryLoader a -> !a

-- | Logarithmic Probabilities
[loaderLogPorbs] :: MemoryLoader a -> !a

-- | Returns
[loaderReturns] :: MemoryLoader a -> !a

-- | Advantages
[loaderAdvantages] :: MemoryLoader a -> !a

-- | Turn Replay memory into chunked data loader
dataLoader :: ReplayMemory Tensor -> Int -> Tensor -> Tensor -> MemoryLoader [Tensor]

-- | How many Trajectories are currently stored in memory
loaderLength :: MemoryLoader [Tensor] -> Int
instance GHC.Classes.Eq a => GHC.Classes.Eq (RPB.ReplayBuffer a)
instance GHC.Show.Show a => GHC.Show.Show (RPB.ReplayBuffer a)
instance GHC.Classes.Eq a => GHC.Classes.Eq (RPB.PERBuffer a)
instance GHC.Show.Show a => GHC.Show.Show (RPB.PERBuffer a)
instance GHC.Classes.Eq a => GHC.Classes.Eq (RPB.ReplayMemory a)
instance GHC.Show.Show a => GHC.Show.Show (RPB.ReplayMemory a)
instance GHC.Classes.Eq a => GHC.Classes.Eq (RPB.MemoryLoader a)
instance GHC.Show.Show a => GHC.Show.Show (RPB.MemoryLoader a)
instance GHC.Base.Functor RPB.MemoryLoader
instance GHC.Base.Functor RPB.ReplayMemory
instance GHC.Base.Functor RPB.PERBuffer
instance GHC.Base.Functor RPB.ReplayBuffer


-- | Proximal Policy Optimization Algorithm
module PPO

-- | Algorithm ID
algorithm :: String

-- | PPO Agent
data Agent
Agent :: ActorNet -> CriticNet -> IndependentTensor -> Adam -> Agent

-- | Policy φ
[φ] :: Agent -> ActorNet

-- | Critic θ
[θ] :: Agent -> CriticNet

-- | Standard Deviation (Continuous)
[logStd] :: Agent -> IndependentTensor

-- | Joint Optimzier
[optim] :: Agent -> Adam

-- | Agent constructor
mkAgent :: Int -> Int -> IO Agent

-- | Save an Agent Checkpoint
saveAgent :: String -> Agent -> IO ()

-- | Load an Agent Checkpoint
loadAgent :: String -> Int -> Int -> Int -> IO Agent

-- | Actor Network Forward Pass depending on <a>actionSpace</a>
π :: ActorNet -> Tensor -> Tensor

-- | Critic Network Forward Pass
q :: CriticNet -> Tensor -> Tensor

-- | Train Proximal Policy Optimization Agent on Environment
train :: Int -> Int -> HymURL -> TrackingURI -> IO Agent
instance GHC.Classes.Eq PPO.ActorNetSpec
instance GHC.Show.Show PPO.ActorNetSpec
instance GHC.Classes.Eq PPO.CriticNetSpec
instance GHC.Show.Show PPO.CriticNetSpec
instance Torch.NN.Parameterized PPO.ActorNet
instance GHC.Show.Show PPO.ActorNet
instance GHC.Generics.Generic PPO.ActorNet
instance Torch.NN.Parameterized PPO.CriticNet
instance GHC.Show.Show PPO.CriticNet
instance GHC.Generics.Generic PPO.CriticNet
instance GHC.Show.Show PPO.Agent
instance GHC.Generics.Generic PPO.Agent
instance Torch.NN.Randomizable PPO.CriticNetSpec PPO.CriticNet
instance Torch.NN.Randomizable PPO.ActorNetSpec PPO.ActorNet


-- | Soft Actor Critic Algorithm Defaults
module SAC.Defaults

-- | Algorithm ID
algorithm :: String

-- | Print verbose debug output
verbose :: Bool

-- | Replay Buffer Type
bufferType :: Buffer

-- | How many steps to take in env
numSteps :: Int

-- | How many gradient update steps
numEpochs :: Int

-- | Total Number of iterations
numIterations :: Int

-- | Early stop criterion
earlyStop :: Tensor

-- | Size of the batches during epoch
batchSize :: Int

-- | Random seed for reproducability
rngSeed :: Int

-- | Maximum time to cut off
maxTime :: Float

-- | ACE Identifier of the Environment
aceId :: String

-- | PDK/Technology backend of the ACE Environment
aceBackend :: String

-- | ACE Environment variant
aceVariant :: Int

-- | Discount Factor
γ :: Tensor

-- | Smoothing Coefficient
τ :: Tensor

-- | Action Noise
εNoise :: Tensor

-- | Temperature Parameter
αConst :: Float

-- | Lower Variance Clipping
σMin :: Float

-- | Upper Variance Clipping
σMax :: Float

-- | Reward Scaling Factor
rewardScale :: Float

-- | Reward Scaling Factor
ρ :: Tensor

-- | Update Step frequency
d :: Int

-- | Priority update factor
εConst :: Tensor

-- | Initial weights
wInit :: Float

-- | Learning Rate for Actor / Policy
ηπ :: Tensor

-- | Learning Rate for Critic(s)
ηq :: Tensor

-- | Learning Rate for Alpha
ηα :: Tensor

-- | Betas
β1 :: Float

-- | Betas
β2 :: Float

-- | Maximum size of Replay Buffer
bufferSize :: Int

-- | Powerlaw Exponent
αStart :: Float

-- | Weight Exponent
βStart :: Float

-- | Weight Exponent Delay
βFrames :: Int


-- | Soft Actor Critic Algorithm Defaults
module SAC

-- | Algorithm ID
algorithm :: String

-- | SAC Agent
data Agent
Agent :: ActorNet -> CriticNet -> CriticNet -> CriticNet -> CriticNet -> Adam -> Adam -> Adam -> Float -> IndependentTensor -> Adam -> Agent

-- | Actor policy φ
[φ] :: Agent -> ActorNet

-- | Online Critic θ1
[θ1] :: Agent -> CriticNet

-- | Online Critic θ2
[θ2] :: Agent -> CriticNet

-- | Target Critic θ'1
[θ1'] :: Agent -> CriticNet

-- | Target Critic θ'2
[θ2'] :: Agent -> CriticNet

-- | Policy Optimizer
[φOptim] :: Agent -> Adam

-- | Critic 1 Optimizer
[θ1Optim] :: Agent -> Adam

-- | Critic 2 Optimizer
[θ2Optim] :: Agent -> Adam

-- | Target Entropy
[h'] :: Agent -> Float

-- | Temperature Coefficient
[αLog] :: Agent -> IndependentTensor

-- | Alpha Optimizer
[αOptim] :: Agent -> Adam

-- | Agent constructor
mkAgent :: Int -> Int -> IO Agent

-- | Save an Agent Checkpoint
saveAgent :: String -> Agent -> IO ()

-- | Load an Agent Checkpoint
loadAgent :: String -> Int -> Int -> Int -> IO Agent

-- | Actor Network Forward Pass
π :: ActorNet -> Tensor -> (Tensor, Tensor)

-- | Critic Network Forward Pass
q :: CriticNet -> Tensor -> Tensor -> Tensor

-- | Convenience Function
q' :: CriticNet -> CriticNet -> Tensor -> Tensor -> Tensor

-- | Get an Action (no grad)
act :: Agent -> Tensor -> IO Tensor

-- | Get an action and log probs (grad)
evaluate :: Agent -> Tensor -> Tensor -> IO (Tensor, Tensor)

-- | Train Soft Actor Critic Agent on Environment
train :: Int -> Int -> HymURL -> TrackingURI -> IO Agent
instance GHC.Classes.Eq SAC.ActorNetSpec
instance GHC.Show.Show SAC.ActorNetSpec
instance GHC.Classes.Eq SAC.CriticNetSpec
instance GHC.Show.Show SAC.CriticNetSpec
instance Torch.NN.Parameterized SAC.ActorNet
instance GHC.Show.Show SAC.ActorNet
instance GHC.Generics.Generic SAC.ActorNet
instance Torch.NN.Parameterized SAC.CriticNet
instance GHC.Show.Show SAC.CriticNet
instance GHC.Generics.Generic SAC.CriticNet
instance GHC.Show.Show SAC.Agent
instance GHC.Generics.Generic SAC.Agent
instance Torch.NN.Randomizable SAC.CriticNetSpec SAC.CriticNet
instance Torch.NN.Randomizable SAC.ActorNetSpec SAC.ActorNet


-- | Twin Delayed Deep Deterministic Policy Gradient Algorithm Defaults
module TD3.Defaults

-- | Algorithm ID
algorithm :: String

-- | Print verbose debug output
verbose :: Bool

-- | Number of episodes to play
numEpisodes :: Int

-- | Horizon T
numIterations :: Int

-- | Number of Steps to take with policy
numSteps :: Int

-- | Number of epochs to train
numEpochs :: Int

-- | Early stop criterion
earlyStop :: Tensor

-- | Mini batch of N transistions
batchSize :: Int

-- | Random seed for reproducability
rngSeed :: Int

-- | ACE Identifier of the Environment
aceId :: String

-- | PDK/Technology backend of the ACE Environment
aceBackend :: String

-- | ACE Environment variant
aceVariant :: Int

-- | Policy and Target Critic Update Delay
d :: Int

-- | Noise clipping
c :: Float

-- | Discount Factor
γ :: Tensor

-- | Avantage Factor
τ :: Tensor

-- | Sampling Noise as Tensor
σ :: Tensor

-- | Decay Period
decayPeriod :: Int

-- | Noise Clipping Minimum
σMin :: Float

-- | Noise Clipping Maximuxm
σMax :: Float

-- | Initial weights
wInit :: Float

-- | Learning Rate
ηθ :: Tensor

-- | Learning Rate
ηφ :: Tensor

-- | Betas
β1 :: Float

-- | Betas
β2 :: Float

-- | Replay Buffer Size
bufferSize :: Int

-- | Initial sample collecting period
warmupPeriode :: Int


-- | Twin Delayed Deep Deterministic Policy Gradient Algorithm
module TD3

-- | Algorithm ID
algorithm :: String

-- | TD3 Agent
data Agent
Agent :: ActorNet -> ActorNet -> CriticNet -> CriticNet -> CriticNet -> CriticNet -> Adam -> Adam -> Adam -> Agent

-- | Online Policy φ
[φ] :: Agent -> ActorNet

-- | Target Policy φ'
[φ'] :: Agent -> ActorNet

-- | Online Critic θ1
[θ1] :: Agent -> CriticNet

-- | Online Critic θ2
[θ2] :: Agent -> CriticNet

-- | Target Critic θ1
[θ1'] :: Agent -> CriticNet

-- | Target Critic θ2
[θ2'] :: Agent -> CriticNet

-- | Policy Optimizer
[φOptim] :: Agent -> Adam

-- | Critic 1 Optimizer
[θ1Optim] :: Agent -> Adam

-- | Critic 2 Optimizer
[θ2Optim] :: Agent -> Adam

-- | Agent constructor
mkAgent :: Int -> Int -> IO Agent

-- | Save an Agent Checkpoint
saveAgent :: String -> Agent -> IO ()

-- | Load an Agent Checkpoint
loadAgent :: String -> Int -> Int -> Int -> IO Agent

-- | Actor Network Forward Pass
π :: ActorNet -> Tensor -> Tensor

-- | Critic Network Forward Pass
q :: CriticNet -> Tensor -> Tensor -> Tensor

-- | Convenience Function
q' :: CriticNet -> CriticNet -> Tensor -> Tensor -> Tensor

-- | Add Exploration Noise to Action
addNoise :: Int -> Tensor -> IO Tensor

-- | Train Twin Delayed Deep Deterministic Policy Gradient Agent on
--   Environment
train :: Int -> Int -> HymURL -> TrackingURI -> IO Agent
instance GHC.Classes.Eq TD3.ActorNetSpec
instance GHC.Show.Show TD3.ActorNetSpec
instance GHC.Classes.Eq TD3.CriticNetSpec
instance GHC.Show.Show TD3.CriticNetSpec
instance Torch.NN.Parameterized TD3.ActorNet
instance GHC.Show.Show TD3.ActorNet
instance GHC.Generics.Generic TD3.ActorNet
instance Torch.NN.Parameterized TD3.CriticNet
instance GHC.Show.Show TD3.CriticNet
instance GHC.Generics.Generic TD3.CriticNet
instance GHC.Show.Show TD3.Agent
instance GHC.Generics.Generic TD3.Agent
instance Torch.NN.Randomizable TD3.CriticNetSpec TD3.CriticNet
instance Torch.NN.Randomizable TD3.ActorNetSpec TD3.ActorNet
