-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | HaskTorch Reinforcement Learning Agents for GACE
--   
--   Please see the README on GitHub at
--   <a>https://github.com/augustunderground/edelwace#readme</a>
@package edelwace
@version 0.1.0.0


-- | Utility and Helper functions for EDELWACE
module Lib

-- | Swaps the arguments of HaskTorch's foldLoop around
foldLoop' :: Int -> (a -> Int -> IO a) -> a -> IO a

-- | Because snake_case sucks
nanToNum :: Float -> Float -> Float -> Tensor -> Tensor

-- | Default limits for <a>nanToNum</a>
nanToNum' :: Tensor -> Tensor

-- | Default limits for <a>nanToNum</a> (0.0)
nanToNum'' :: Tensor -> Tensor

-- | GPU Tensor filled with Float value
fullLike' :: Tensor -> Float -> Tensor

-- | Select index with [Int] from GPU tensor
indexSelect'' :: Int -> [Int] -> Tensor -> Tensor

-- | Apply same function to both Left and Right
both :: (a -> b) -> Either a a -> b

-- | Apply same function to both Left and Right and put back into Either
both' :: (a -> b) -> Either a a -> Either b b

-- | Range from 0 to n - 1
range :: Int -> [Int]

-- | First of triple
fst' :: (a, b, c) -> a

-- | Current Timestamp as formatted string
currentTimeStamp :: String -> IO String

-- | Current Timestamp with default formatting: "%Y%m%d-%H%M%S"
currentTimeStamp' :: IO String

-- | Create a model archive directory for the given algorithm
createModelArchiveDir :: String -> IO String

-- | Calculate weight Limits based on Layer Dimensions
weightLimit :: Linear -> Float

-- | Type of weight initialization
data Initializer

-- | Normally distributed weights
Normal :: Initializer

-- | Uniformally distributed weights
Uniform :: Initializer

-- | Using T.xavierNormal
XavierNormal :: Initializer

-- | Using T.xavierUniform
XavierUniform :: Initializer

-- | Using T.kaimingNormal
KaimingNormal :: Initializer

-- | Using T.kaimingUniform
KaimingUniform :: Initializer
Dirac :: Initializer
Eye :: Initializer
Ones :: Initializer
Zeros :: Initializer
Constant :: Initializer

-- | Weights for a layer given limits and dimensions.
initWeights :: Initializer -> Float -> Float -> [Int] -> IO IndependentTensor

-- | Initialize Weights of Linear Layer
weightInit :: Initializer -> Float -> Float -> Linear -> IO Linear

-- | Initialize weights uniformally given upper and lower bounds
weightInitUniform :: Float -> Float -> Linear -> IO Linear

-- | Initialize weights uniformally based on Fan In
weightInitUniform' :: Linear -> IO Linear

-- | Initialize weights normally given mean and std bounds
weightInitNormal :: Float -> Float -> Linear -> IO Linear

-- | Initialize weights normally based on Fan In
weightInitNormal' :: Linear -> IO Linear

-- | Softly update parameters from Online Net to Target Net
softUpdate :: Tensor -> Tensor -> Tensor -> Tensor

-- | Softly copy parameters from Online Net to Target Net
softSync :: Parameterized f => Tensor -> f -> f -> IO f

-- | Hard Copy of Parameter from one net to the other
copySync :: Parameterized f => f -> f -> f

-- | GPU 1
gpu :: Device

-- | CPU 0
cpu :: Device

-- | Default Tensor Data Type
dataType :: DType

-- | Convert an Array to a Tensor
toTensor :: TensorLike a => a -> Tensor

-- | Convert an Array to a Tensor
toIntTensor :: TensorLike a => a -> Tensor

-- | Create an empty Float Tensor on GPU
emptyTensor :: Tensor

-- | Convert a Scalar to a Tensor
toScalar :: Float -> Tensor

-- | Convert model to Double on GPU
toDoubleGPU :: forall a. HasTypes a Tensor => a -> a

-- | Convert model to Float on CPU
toFloatGPU :: forall a. HasTypes a Tensor => a -> a

-- | Convert model to Float on CPU
toFloatCPU :: forall a. HasTypes a Tensor => a -> a

-- | Generate a Tensor of random Integers
randomInts :: Int -> Int -> Int -> IO Tensor

-- | Generate Normally Distributed Random values given dimensions
normal' :: [Int] -> IO Tensor

-- | Generate Normally Distributed Random values given μs and σs
normal :: Tensor -> Tensor -> IO Tensor

-- | Generate Uniformally distributed values in a given range
uniform' :: [Int] -> Float -> Float -> IO Tensor

-- | Info object gotten form stepping
data Info
Info :: ![String] -> ![String] -> Info

-- | Observation Keys
[observations] :: Info -> ![String]

-- | Action Keys
[actions] :: Info -> ![String]

-- | Single Environment Step
data Step
Step :: ![Float] -> !Float -> !Bool -> !Info -> Step

-- | Observation Vector
[observation] :: Step -> ![Float]

-- | Reward Scalar
[reward] :: Step -> !Float

-- | Terminal Indicator
[done] :: Step -> !Bool

-- | Info
[info] :: Step -> !Info

-- | Base Route to Hym Server
type HymURL = String

-- | Possible Action Spaces
data ActionSpace

-- | Continuous Action Space
Continuous :: ActionSpace

-- | Discrete Action Space
Discrete :: ActionSpace

-- | Convert a Map to a Tensor where Pool index is a dimension
mapToTensor :: Map Int [Float] -> Tensor

-- | Convert Tensor to Map (Continuous action spaces)
tensorToMap :: Tensor -> Map Int [Float]

-- | Convert Tensor to Map (Discrete action spaces)
tensorToMap' :: Tensor -> Map Int Int

-- | Convert the Pooled Step Map to a Tuple
stepsToTuple :: Map Int Step -> (Tensor, Tensor, Tensor, [Info])

-- | Generic HTTP GET Request to Hym Server
hymGet :: HymURL -> String -> IO ByteString

-- | Send a POST Request to a Hym Server
hymPost :: HymURL -> String -> Value -> IO ByteString

-- | Convert a JSON Response from an ACE Server to a Map
hymPoolMap :: HymURL -> String -> IO (Map Int (Map String Float))

-- | Convert a JSON Response from an ACE Server to a Float-List
hymPoolList :: HymURL -> String -> IO (Map Int [Float])

-- | Convert a JSON Response from an ACE Server to a String-List
hymPoolList' :: HymURL -> String -> IO (Map Int [String])

-- | Reset Pooled Environments on a Hym server
hymPoolReset :: HymURL -> IO (Map Int [Float])

-- | Get Random Actions from all Pooled Environments
hymPoolRandomAction :: HymURL -> IO (Map Int [Float])

-- | Perform Random Actions in all Pooled Environments
hymPoolRandomStep :: HymURL -> IO (Map Int Step)

-- | Take Steps in All Environments (Continuous)
hymPoolStep :: HymURL -> Map Int [Float] -> IO (Map Int Step)

-- | Take Steps in All Environments (Discrete)
hymPoolStep' :: HymURL -> Map Int Int -> IO (Map Int Step)

-- | Generate URL to a Hym-GACE server from meta information
aceURL :: String -> String -> String -> String -> String -> HymURL

-- | Generate URL to a Hym-Gym server from meta information
gymURL :: String -> String -> String -> String -> HymURL

-- | Send a GET Request to a GACE Server Obtain the Target of Pooled GACE
--   Environments
acePoolTarget :: HymURL -> IO (Map Int (Map String Float))

-- | Action Keys from GACE Server
acePoolActKeys :: HymURL -> IO (Map Int [String])

-- | Observation Keys from GACE Server
acePoolObsKeys :: HymURL -> IO (Map Int [String])

-- | Get the SHACE logging path as a dict
shaceLogPath' :: HymURL -> IO (Map String String)

-- | Get the SHACE logging path
shaceLogPath :: HymURL -> IO String

-- | Reset a Vectorized Environment Pool
resetPool :: HymURL -> IO Tensor

-- | Reset selected Environments from Pool
resetPool' :: HymURL -> Tensor -> IO Tensor

-- | Shorthand for getting keys of pooled same envs
actKeysPool :: HymURL -> IO [String]

-- | Shorthand for getting keys of pooled same envs
obsKeysPool :: HymURL -> IO [String]

-- | Number of Environments in Pool
numEnvsPool :: HymURL -> IO Int

-- | Get Info without stepping
infoPool :: HymURL -> IO Info

-- | Get Targets for all envs in Pool
targetPool :: HymURL -> IO Tensor

-- | Get Targets for all envs in Pool and process them
targetPool' :: HymURL -> IO Tensor

-- | Step in a Control Environment
stepPool :: HymURL -> Tensor -> IO (Tensor, Tensor, Tensor, [Info])

-- | Step in a Discrete Environment
stepPool' :: HymURL -> Tensor -> IO (Tensor, Tensor, Tensor, [Info])

-- | Take a random Step an Environment
randomStepPool :: HymURL -> IO (Tensor, Tensor, Tensor, [Info])

-- | Get a set of random actions from the current environment
randomActionPool :: HymURL -> IO Tensor

-- | Optimizer moments at given prefix
saveOptim :: Adam -> FilePath -> IO ()

-- | Load Optimizer State
loadOptim :: Int -> Float -> Float -> FilePath -> IO Adam

-- | Create Boolean Mask Tensor from list of indices.
boolMask :: Int -> [Int] -> Tensor

-- | Create a Boolean Mask Tensor from index Tensor
boolMask' :: Int -> Tensor -> Tensor

-- | Process Targets for HER
processTarget :: Map Int (Map String Float) -> Tensor

-- | Process for HER returns processed observations, the target and the
--   augmented target
processGace'' :: Tensor -> Info -> (Tensor, Tensor, Tensor)

-- | Standardize state over all parallel envs
processGace' :: Tensor -> Info -> Tensor

-- | Process / Sanitize the Observations from GACE
processGace :: Tensor -> Info -> Tensor

-- | Scale reward to center
scaleRewards :: Tensor -> Float -> Tensor

-- | Sanatize JSON for MLFlow: Names may only contain alphanumerics,
--   underscores (_), dashes (-), periods (.), spaces ( ), and slashes (/).
sanatizeJSON :: Char -> Char

-- | Data Logging to MLFlow Trackign Server
data Tracker
Tracker :: TrackingURI -> ExperimentID -> String -> Map String RunID -> Tracker

-- | Tracking Server URI
[uri] :: Tracker -> TrackingURI

-- | Experiment ID
[experimentId] :: Tracker -> ExperimentID

-- | Experiment Name
[experimentName] :: Tracker -> String

-- | Run IDs
[runIds] :: Tracker -> Map String RunID

-- | Retrieve a run ID
runId :: Tracker -> String -> RunID

-- | Make new Tracker given a Tracking Server URI
mkTracker :: TrackingURI -> String -> IO Tracker

-- | Make new Tracker given a Hostname and Port
mkTracker' :: String -> Int -> String -> IO Tracker

-- | Create a new Experiment with rng suffix
newExperiment :: Tracker -> String -> IO Tracker

-- | Create a new Experiment
newExperiment' :: Tracker -> String -> IO Tracker

-- | Create a new run with a set of given paramters
newRuns :: Tracker -> [String] -> [Param] -> IO Tracker

-- | New run with algorithm id and #envs as log params
newRuns' :: Int -> Tracker -> IO Tracker

-- | End a run
endRun :: String -> Tracker -> IO Tracker

-- | End all runs of a Tracker
endRuns :: Tracker -> IO Tracker

-- | End all runs and discard tracker
endRuns' :: Tracker -> IO ()

-- | Write Loss to Tracking Server
trackLoss :: Tracker -> Int -> String -> Float -> IO (Response ByteString)

-- | Write Reward to Tracking Server
trackReward :: Tracker -> Int -> Tensor -> IO ()

-- | Filter Performance of all envs
filterPerformance :: Map Int (Map String Float) -> [String] -> Map Int (Map String Float)

-- | Write Current state of the Environment to Trackign Server
trackEnvState :: Tracker -> HymURL -> Int -> IO ()
instance GHC.Show.Show Lib.Info
instance GHC.Generics.Generic Lib.Info
instance GHC.Show.Show Lib.Step
instance GHC.Generics.Generic Lib.Step
instance GHC.Classes.Eq Lib.ActionSpace
instance GHC.Show.Show Lib.ActionSpace
instance GHC.Show.Show Lib.Tracker
instance Data.Aeson.Types.FromJSON.FromJSON Lib.Step
instance Data.Aeson.Types.ToJSON.ToJSON Lib.Step
instance Data.Aeson.Types.FromJSON.FromJSON Lib.Info
instance Data.Aeson.Types.ToJSON.ToJSON Lib.Info

module Normal

-- | Creates a normal (also called Gaussian) distribution parameterized by
--   <a>loc</a> and <a>scale</a>. Example:
--   
--   <pre>
--   &gt;&gt;&gt; m = Normal (asTensor ([0.0] :: [Float])) (asTensor ([1.0] :: [Float]))
--   
--   &gt;&gt;&gt; sample m []
--   Tensor Float [1] [-0.1205   ]
--   </pre>
data Normal
Normal :: Tensor -> Tensor -> Normal

-- | mean of the distribution (often referred to as mu)
[loc] :: Normal -> Tensor

-- | standard deviation of the distribution (often referred to as sigma)
[scale] :: Normal -> Tensor

-- | PDF for given Normal Distribution
prob :: Normal -> Tensor -> Tensor
instance GHC.Show.Show Normal.Normal
instance Torch.Distributions.Distribution.Distribution Normal.Normal


-- | Replay Buffers and Memory Loaders
module RPB

-- | Indicate Buffer Type to be used by Algorithm
data BufferType

-- | Normal Replay Buffer (SAC, TD3)
RPB :: BufferType

-- | Prioritized Experience Replay (SAC)
PER :: BufferType

-- | PPO Style replay Memory (PPO)
MEM :: BufferType

-- | Emphasizing Recent Experience (SAC)
ERE :: BufferType

-- | Hindsight Experience Replay (TD3)
HER :: BufferType
instance GHC.Classes.Eq RPB.BufferType
instance GHC.Show.Show RPB.BufferType


-- | Proximal Policy Optimization Algorithm Defaults
module PPO.Defaults

-- | Algorithm ID
algorithm :: String

-- | Print verbose debug output
verbose :: Bool

-- | Replay Buffer Type
bufferType :: BufferType

-- | Number of episodes to play
numEpisodes :: Int

-- | How many steps to take in env
numSteps :: Int

-- | How many gradient update steps
numEpochs :: Int

-- | Number of iterations
numIterations :: Int

-- | Early stop criterion
earlyStop :: Tensor

-- | Size of the batches during epoch
batchSize :: Int

-- | Random seed for reproducability
rngSeed :: Int

-- | Maximum time to cut off
maxTime :: Float

-- | ACE Identifier of the Environment
aceId :: String

-- | PDK/Technology backend of the ACE Environment
aceBackend :: String

-- | ACE Environment variant
aceVariant :: Int

-- | Discrete or Continuous action space
actionSpace :: ActionSpace

-- | Scale for reward centering
rewardScale :: Float

-- | Factor for clipping
ε :: Float

-- | Factor in loss function
δ :: Tensor

-- | Discount Factor
γ :: Tensor

-- | Avantage Factor
τ :: Tensor

-- | Initial weights
wInit :: Float

-- | Learning Rate
η :: Tensor

-- | Betas
β1 :: Float

-- | Betas
β2 :: Float


-- | Replay Buffers and Memory Loaders
module RPB.MEM

-- | Replay Memory
data Buffer a
Buffer :: !a -> !a -> !a -> !a -> !a -> !a -> Buffer a

-- | States
[states] :: Buffer a -> !a

-- | Action
[actions] :: Buffer a -> !a

-- | Logarithmic Probability
[logProbs] :: Buffer a -> !a

-- | Rewards
[rewards] :: Buffer a -> !a

-- | Values
[values] :: Buffer a -> !a

-- | Terminal Mask
[masks] :: Buffer a -> !a

-- | Create a new, empty Buffer on the GPU
mkBuffer :: Buffer Tensor

-- | How many Trajectories are currently stored in memory
size :: Buffer Tensor -> Int

-- | Push new memories into Buffer
push :: Buffer Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Buffer Tensor

-- | Pushing one buffer into another one
push' :: Buffer Tensor -> Buffer Tensor -> Buffer Tensor

-- | Generalized Advantage Estimate
gae :: Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor

-- | Memory Data Loader
data Loader a
Loader :: !a -> !a -> !a -> !a -> !a -> Loader a

-- | States
[states'] :: Loader a -> !a

-- | Actions
[actions'] :: Loader a -> !a

-- | Logarithmic Probabilities
[logProbs'] :: Loader a -> !a

-- | Returns
[returns'] :: Loader a -> !a

-- | Advantages
[advantages'] :: Loader a -> !a

-- | Turn Replay memory into chunked data loader
mkLoader :: Buffer Tensor -> Int -> Tensor -> Tensor -> Loader [Tensor]

-- | How many Trajectories are currently stored in memory
size' :: Loader [Tensor] -> Int
instance GHC.Classes.Eq a => GHC.Classes.Eq (RPB.MEM.Buffer a)
instance GHC.Show.Show a => GHC.Show.Show (RPB.MEM.Buffer a)
instance GHC.Classes.Eq a => GHC.Classes.Eq (RPB.MEM.Loader a)
instance GHC.Show.Show a => GHC.Show.Show (RPB.MEM.Loader a)
instance GHC.Base.Functor RPB.MEM.Loader
instance GHC.Base.Functor RPB.MEM.Buffer


-- | Proximal Policy Optimization Algorithm
module PPO

-- | Algorithm ID
algorithm :: String

-- | Actor Network Specification
data ActorNetSpec
ActorNetSpec :: Int -> Int -> ActorNetSpec
[pObsDim] :: ActorNetSpec -> Int
[pActDim] :: ActorNetSpec -> Int

-- | Critic Network Specification
newtype CriticNetSpec
CriticNetSpec :: Int -> CriticNetSpec
[qObsDim] :: CriticNetSpec -> Int

-- | Actor Network Architecture
data ActorNet
ActorNet :: Linear -> Linear -> Linear -> ActorNet
[pLayer0] :: ActorNet -> Linear
[pLayer1] :: ActorNet -> Linear
[pLayer2] :: ActorNet -> Linear

-- | Critic Network Architecture
data CriticNet
CriticNet :: Linear -> Linear -> Linear -> CriticNet
[qLayer0] :: CriticNet -> Linear
[qLayer1] :: CriticNet -> Linear
[qLayer2] :: CriticNet -> Linear

-- | PPO Agent
data Agent
Agent :: ActorNet -> CriticNet -> IndependentTensor -> Adam -> Agent

-- | Policy φ
[φ] :: Agent -> ActorNet

-- | Critic θ
[θ] :: Agent -> CriticNet

-- | Standard Deviation (Continuous)
[logStd] :: Agent -> IndependentTensor

-- | Joint Optimzier
[optim] :: Agent -> Adam

-- | Agent constructor
mkAgent :: Int -> Int -> IO Agent

-- | Save an Agent Checkpoint
saveAgent :: String -> Agent -> IO ()

-- | Load an Agent Checkpoint
loadAgent :: String -> Int -> Int -> Int -> IO Agent

-- | Actor Network Forward Pass depending on <a>actionSpace</a>
π :: ActorNet -> Tensor -> Tensor

-- | Critic Network Forward Pass
q :: CriticNet -> Tensor -> Tensor

-- | Train Proximal Policy Optimization Agent on Environment
train :: Int -> Int -> HymURL -> TrackingURI -> IO Agent
instance GHC.Classes.Eq PPO.ActorNetSpec
instance GHC.Show.Show PPO.ActorNetSpec
instance GHC.Classes.Eq PPO.CriticNetSpec
instance GHC.Show.Show PPO.CriticNetSpec
instance Torch.NN.Parameterized PPO.ActorNet
instance GHC.Show.Show PPO.ActorNet
instance GHC.Generics.Generic PPO.ActorNet
instance Torch.NN.Parameterized PPO.CriticNet
instance GHC.Show.Show PPO.CriticNet
instance GHC.Generics.Generic PPO.CriticNet
instance GHC.Show.Show PPO.Agent
instance GHC.Generics.Generic PPO.Agent
instance Torch.NN.Randomizable PPO.CriticNetSpec PPO.CriticNet
instance Torch.NN.Randomizable PPO.ActorNetSpec PPO.ActorNet


-- | Replay Buffers and Memory Loaders
module RPB.RPB

-- | Strict Simple/Naive Replay Buffer
data Buffer a
Buffer :: !a -> !a -> !a -> !a -> !a -> Buffer a

-- | States
[states] :: Buffer a -> !a

-- | Actions
[actions] :: Buffer a -> !a

-- | Rewards
[rewards] :: Buffer a -> !a

-- | Next States
[states'] :: Buffer a -> !a

-- | Terminal Mask
[dones] :: Buffer a -> !a

-- | Create a new, empty Buffer on the GPU
mkBuffer :: Buffer Tensor

-- | How many Trajectories are currently stored in memory
size :: Buffer Tensor -> Int

-- | Push new memories into Buffer
push :: Int -> Buffer Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Buffer Tensor

-- | Push one buffer into another one
push' :: Int -> Buffer Tensor -> Buffer Tensor -> Buffer Tensor

-- | Pop numElems from Buffer
pop :: Int -> Buffer Tensor -> Buffer Tensor

-- | Get the given indices from Buffer
sample :: Tensor -> Buffer Tensor -> Buffer Tensor

-- | Uniform random sample from Replay Buffer
sampleIO :: Int -> Buffer Tensor -> IO (Buffer Tensor)
instance GHC.Classes.Eq a => GHC.Classes.Eq (RPB.RPB.Buffer a)
instance GHC.Show.Show a => GHC.Show.Show (RPB.RPB.Buffer a)
instance GHC.Base.Functor RPB.RPB.Buffer


-- | Replay Buffers and Memory Loaders
module RPB.PER

-- | Strict Prioritized Experience Replay Buffer
data Buffer a
Buffer :: Buffer a -> !Tensor -> !Int -> !Float -> !Float -> !Int -> Buffer a

-- | Actual Buffer
[memories] :: Buffer a -> Buffer a

-- | Sample Weights
[priorities] :: Buffer a -> !Tensor

-- | Buffer Capacity
[capacity] :: Buffer a -> !Int

-- | Exponent Alpha
[alpha] :: Buffer a -> !Float

-- | Initial Exponent Beta
[betaStart] :: Buffer a -> !Float

-- | Beta Decay
[betaFrames] :: Buffer a -> !Int

-- | Create an empty PER Buffer
mkBuffer :: Int -> Float -> Float -> Int -> Buffer Tensor

-- | Push new memories in a Buffer
push :: Buffer Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Buffer Tensor

-- | Syntactic Sugar for adding one buffer to another
push' :: Buffer Tensor -> Buffer Tensor -> Buffer Tensor

-- | Take a prioritized sample from the Buffer
sampleIO :: Buffer Tensor -> Int -> Int -> IO (Buffer Tensor, Tensor, Tensor)

-- | Update the Priorities of a Buffer
update :: Buffer Tensor -> Tensor -> Tensor -> Buffer Tensor

-- | Calculate the β exponent at a given frame
betaByFrame :: Float -> Int -> Int -> Float
instance GHC.Classes.Eq a => GHC.Classes.Eq (RPB.PER.Buffer a)
instance GHC.Show.Show a => GHC.Show.Show (RPB.PER.Buffer a)
instance GHC.Base.Functor RPB.PER.Buffer


-- | Replay Buffers and Memory Loaders
module RPB.HER

-- | Hindsight Experience Replay Strategies for choosing Goals
data Strategy

-- | Only Final States are additional targets
Final :: Strategy

-- | Replay with <tt>k</tt> random states encountered so far (basically
--   RPB)
Random :: Strategy

-- | Replay with <tt>k</tt> random states from same episode.
Episode :: Strategy

-- | Replay with <tt>k</tt> random states from same episode, that were
--   observed after
Future :: Strategy

-- | Strict Simple/Naive Replay Buffer
data Buffer a
Buffer :: !a -> !a -> !a -> !a -> !a -> !a -> !a -> Buffer a

-- | States
[states] :: Buffer a -> !a

-- | Actions
[actions] :: Buffer a -> !a

-- | Rewards
[rewards] :: Buffer a -> !a

-- | Next States
[states'] :: Buffer a -> !a

-- | Terminal Mask
[dones] :: Buffer a -> !a

-- | Actual Episode Target
[targets] :: Buffer a -> !a

-- | Augmented Target
[targets'] :: Buffer a -> !a

-- | Create a new, empty HER Buffer on the GPU
mkBuffer :: Buffer Tensor

-- | How many Trajectories are currently stored in memory
size :: Buffer Tensor -> Int

-- | Push new memories into Buffer
push :: Int -> Buffer Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Tensor -> Buffer Tensor

-- | Push one buffer into another one
push' :: Int -> Buffer Tensor -> Buffer Tensor -> Buffer Tensor

-- | Drop everything after done (used for single episode)
drop' :: Buffer Tensor -> Buffer Tensor

-- | Split buffer collected from pool by env
envSplit :: Int -> Buffer Tensor -> [Buffer Tensor]

-- | Get the given indices from Buffer
sample :: Tensor -> Buffer Tensor -> Buffer Tensor

-- | Sample Additional Goals according to Strategy (drop first).
--   <a>Random</a> is basically the same as <a>Episode</a> you just have to
--   give it the entire buffer, not just the episode.
sampleTargets :: Strategy -> Int -> Tensor -> Buffer Tensor -> IO (Buffer Tensor)

-- | Convert HER Buffer to RPB for training
asRPB :: Buffer Tensor -> Buffer Tensor
instance GHC.Classes.Eq RPB.HER.Strategy
instance GHC.Show.Show RPB.HER.Strategy
instance GHC.Classes.Eq a => GHC.Classes.Eq (RPB.HER.Buffer a)
instance GHC.Show.Show a => GHC.Show.Show (RPB.HER.Buffer a)
instance GHC.Base.Functor RPB.HER.Buffer


-- | Replay Buffers and Memory Loaders
module RPB.ERE

-- | Calculate ERE Sampling range cK
samplingRange :: Int -> Int -> Int -> Int -> Float -> Int

-- | Sample for buffer within ERE range
sample :: Buffer Tensor -> Int -> Int -> Int -> Int -> Int -> Float -> IO (Buffer Tensor)

-- | ERE η Annealing during training
anneal :: Float -> Float -> Int -> Int -> Float


-- | Soft Actor Critic Algorithm Defaults
module SAC.Defaults

-- | Algorithm ID
algorithm :: String

-- | Print verbose debug output
verbose :: Bool

-- | Replay Buffer Type
bufferType :: BufferType

-- | How many steps to take in env
numSteps :: Int

-- | How many gradient update steps
numEpochs :: Int

-- | Total Number of iterations, depends on <a>bufferType</a>.
numIterations :: Int

-- | Early stop criterion
earlyStop :: Tensor

-- | Reward Lower Bound
minReward :: Float

-- | Size of the batches during epoch
batchSize :: Int

-- | Random seed for reproducability
rngSeed :: Int

-- | Maximum time to cut off
maxTime :: Float

-- | ACE Identifier of the Environment
aceId :: String

-- | PDK/Technology backend of the ACE Environment
aceBackend :: String

-- | ACE Environment variant
aceVariant :: Int

-- | Discount Factor
γ :: Tensor

-- | Smoothing Coefficient
τ :: Tensor

-- | Action Noise
εNoise :: Tensor

-- | Whether temperature coefficient is fixed or learned (see αInit)
αLearned :: Bool

-- | Temperature Coefficient
αInit :: Tensor

-- | Lower Variance Clipping
σMin :: Float

-- | Upper Variance Clipping
σMax :: Float

-- | Reward Scaling Factor
rewardScale :: Tensor

-- | Reward Scaling Factor
ρ :: Tensor

-- | Update Step frequency
d :: Int

-- | Priority update factor
εConst :: Tensor

-- | Initial weights
wInit :: Float

-- | Learning Rate for Actor / Policy
ηπ :: Tensor

-- | Learning Rate for Critic(s)
ηq :: Tensor

-- | Learning Rate for Alpha
ηα :: Tensor

-- | Betas
β1 :: Float

-- | Betas
β2 :: Float

-- | Maximum size of Replay Buffer
bufferSize :: Int

-- | Powerlaw Exponent
αStart :: Float

-- | Weight Exponent
βStart :: Float

-- | Weight Exponent Delay
βFrames :: Int

-- | Initial η
η0 :: Float

-- | Final η
ηT :: Float

-- | Minimum Sampling Range
cMin :: Int


-- | Soft Actor Critic Algorithm Defaults
module SAC

-- | Algorithm ID
algorithm :: String

-- | Actor Network Specification
data ActorNetSpec
ActorNetSpec :: Int -> Int -> ActorNetSpec
[pObsDim] :: ActorNetSpec -> Int
[pActDim] :: ActorNetSpec -> Int

-- | Critic Network Specification
data CriticNetSpec
CriticNetSpec :: Int -> Int -> CriticNetSpec
[qObsDim] :: CriticNetSpec -> Int
[qActDim] :: CriticNetSpec -> Int

-- | Actor Network Architecture
data ActorNet
ActorNet :: Linear -> Linear -> Linear -> Linear -> ActorNet
[pLayer0] :: ActorNet -> Linear
[pLayer1] :: ActorNet -> Linear
[pLayerμ] :: ActorNet -> Linear
[pLayerσ] :: ActorNet -> Linear

-- | Critic Network Architecture
data CriticNet
CriticNet :: Linear -> Linear -> Linear -> CriticNet
[qLayer0] :: CriticNet -> Linear
[qLayer1] :: CriticNet -> Linear
[qLayer2] :: CriticNet -> Linear

-- | SAC Agent
data Agent
Agent :: ActorNet -> CriticNet -> CriticNet -> CriticNet -> CriticNet -> Adam -> Adam -> Adam -> Float -> IndependentTensor -> Adam -> Agent

-- | Actor policy φ
[φ] :: Agent -> ActorNet

-- | Online Critic θ1
[θ1] :: Agent -> CriticNet

-- | Online Critic θ2
[θ2] :: Agent -> CriticNet

-- | Target Critic θ'1
[θ1'] :: Agent -> CriticNet

-- | Target Critic θ'2
[θ2'] :: Agent -> CriticNet

-- | Policy Optimizer
[φOptim] :: Agent -> Adam

-- | Critic 1 Optimizer
[θ1Optim] :: Agent -> Adam

-- | Critic 2 Optimizer
[θ2Optim] :: Agent -> Adam

-- | Target Entropy
[h'] :: Agent -> Float

-- | Temperature Coefficient
[αLog] :: Agent -> IndependentTensor

-- | Alpha Optimizer
[αOptim] :: Agent -> Adam

-- | Agent constructor
mkAgent :: Int -> Int -> IO Agent

-- | Save an Agent Checkpoint
saveAgent :: String -> Agent -> IO ()

-- | Load an Agent Checkpoint
loadAgent :: String -> Int -> Int -> Int -> IO Agent

-- | Actor Network Forward Pass
π :: ActorNet -> Tensor -> (Tensor, Tensor)

-- | Critic Network Forward Pass
q :: CriticNet -> Tensor -> Tensor -> Tensor

-- | Convenience Function
q' :: CriticNet -> CriticNet -> Tensor -> Tensor -> Tensor

-- | Perform a completely random action for a given state
actRandom :: Agent -> Tensor -> IO Tensor

-- | Get an Action (no grad)
act :: Agent -> Tensor -> IO Tensor

-- | Get an action and log probs (grad)
evaluate :: Agent -> Tensor -> Tensor -> IO (Tensor, Tensor)

-- | Train Soft Actor Critic Agent on Environment
train :: Int -> Int -> HymURL -> TrackingURI -> IO Agent
instance GHC.Classes.Eq SAC.ActorNetSpec
instance GHC.Show.Show SAC.ActorNetSpec
instance GHC.Classes.Eq SAC.CriticNetSpec
instance GHC.Show.Show SAC.CriticNetSpec
instance Torch.NN.Parameterized SAC.ActorNet
instance GHC.Show.Show SAC.ActorNet
instance GHC.Generics.Generic SAC.ActorNet
instance Torch.NN.Parameterized SAC.CriticNet
instance GHC.Show.Show SAC.CriticNet
instance GHC.Generics.Generic SAC.CriticNet
instance GHC.Show.Show SAC.Agent
instance GHC.Generics.Generic SAC.Agent
instance Torch.NN.Randomizable SAC.CriticNetSpec SAC.CriticNet
instance Torch.NN.Randomizable SAC.ActorNetSpec SAC.ActorNet


-- | Twin Delayed Deep Deterministic Policy Gradient Algorithm Defaults
module TD3.Defaults

-- | Algorithm ID
algorithm :: String

-- | Print verbose debug output
verbose :: Bool

-- | Number of episodes to play
numEpisodes :: Int

-- | Horizon T
numIterations :: Int

-- | Number of Steps to take with policy
numSteps :: Int

-- | Number of epochs to train
numEpochs :: Int

-- | Early stop criterion
earlyStop :: Tensor

-- | Mini batch of N transistions
batchSize :: Int

-- | Random seed for reproducability
rngSeed :: Int

-- | ACE Identifier of the Environment
aceId :: String

-- | PDK/Technology backend of the ACE Environment
aceBackend :: String

-- | ACE Environment variant
aceVariant :: Int

-- | Policy and Target Critic Update Delay
d :: Int

-- | Noise clipping
c :: Float

-- | Discount Factor
γ :: Tensor

-- | Avantage Factor
τ :: Tensor

-- | Sampling Noise as Tensor
σ :: Tensor

-- | Decay Period
decayPeriod :: Int

-- | Noise Clipping Minimum
σMin :: Float

-- | Noise Clipping Maximuxm
σMax :: Float

-- | Initial weights
wInit :: Float

-- | Learning Rate
ηθ :: Tensor

-- | Learning Rate
ηφ :: Tensor

-- | Betas
β1 :: Float

-- | Betas
β2 :: Float

-- | Replay Buffer Type
bufferType :: BufferType

-- | Replay Buffer Size
bufferSize :: Int

-- | Initial sample collecting period
warmupPeriode :: Int

-- | Target Sampling Strategy
samplingStrategy :: Strategy

-- | Number of Additional Targets to sample
k :: Int

-- | Error Tolerance for Target / Reward Calculation
relTol :: Tensor


-- | Twin Delayed Deep Deterministic Policy Gradient Algorithm
module TD3

-- | Algorithm ID
algorithm :: String

-- | Actor Network Specification
data ActorNetSpec
ActorNetSpec :: Int -> Int -> ActorNetSpec
[pObsDim] :: ActorNetSpec -> Int
[pActDim] :: ActorNetSpec -> Int

-- | Critic Network Specification
data CriticNetSpec
CriticNetSpec :: Int -> Int -> CriticNetSpec
[qObsDim] :: CriticNetSpec -> Int
[qActDim] :: CriticNetSpec -> Int

-- | Actor Network Architecture
data ActorNet
ActorNet :: Linear -> Linear -> Linear -> ActorNet
[pLayer0] :: ActorNet -> Linear
[pLayer1] :: ActorNet -> Linear
[pLayer2] :: ActorNet -> Linear

-- | Critic Network Architecture
data CriticNet
CriticNet :: Linear -> Linear -> Linear -> CriticNet
[qLayer0] :: CriticNet -> Linear
[qLayer1] :: CriticNet -> Linear
[qLayer2] :: CriticNet -> Linear

-- | TD3 Agent
data Agent
Agent :: ActorNet -> ActorNet -> CriticNet -> CriticNet -> CriticNet -> CriticNet -> Adam -> Adam -> Adam -> Agent

-- | Online Policy φ
[φ] :: Agent -> ActorNet

-- | Target Policy φ'
[φ'] :: Agent -> ActorNet

-- | Online Critic θ1
[θ1] :: Agent -> CriticNet

-- | Online Critic θ2
[θ2] :: Agent -> CriticNet

-- | Target Critic θ1
[θ1'] :: Agent -> CriticNet

-- | Target Critic θ2
[θ2'] :: Agent -> CriticNet

-- | Policy Optimizer
[φOptim] :: Agent -> Adam

-- | Critic 1 Optimizer
[θ1Optim] :: Agent -> Adam

-- | Critic 2 Optimizer
[θ2Optim] :: Agent -> Adam

-- | Agent constructor
mkAgent :: Int -> Int -> IO Agent

-- | Save an Agent Checkpoint
saveAgent :: String -> Agent -> IO ()

-- | Load an Agent Checkpoint
loadAgent :: String -> Int -> Int -> Int -> IO Agent

-- | Actor Network Forward Pass
π :: ActorNet -> Tensor -> Tensor

-- | Critic Network Forward Pass
q :: CriticNet -> Tensor -> Tensor -> Tensor

-- | Convenience Function
q' :: CriticNet -> CriticNet -> Tensor -> Tensor -> Tensor

-- | Add Exploration Noise to Action
addNoise :: Int -> Tensor -> IO Tensor

-- | Train Twin Delayed Deep Deterministic Policy Gradient Agent on
--   Environment
train :: Int -> Int -> HymURL -> TrackingURI -> IO Agent
instance GHC.Classes.Eq TD3.ActorNetSpec
instance GHC.Show.Show TD3.ActorNetSpec
instance GHC.Classes.Eq TD3.CriticNetSpec
instance GHC.Show.Show TD3.CriticNetSpec
instance Torch.NN.Parameterized TD3.ActorNet
instance GHC.Show.Show TD3.ActorNet
instance GHC.Generics.Generic TD3.ActorNet
instance Torch.NN.Parameterized TD3.CriticNet
instance GHC.Show.Show TD3.CriticNet
instance GHC.Generics.Generic TD3.CriticNet
instance GHC.Show.Show TD3.Agent
instance GHC.Generics.Generic TD3.Agent
instance Torch.NN.Randomizable TD3.CriticNetSpec TD3.CriticNet
instance Torch.NN.Randomizable TD3.ActorNetSpec TD3.ActorNet
